{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d63db95",
   "metadata": {},
   "source": [
    "# Information retrieval using word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2cb5d725",
   "metadata": {},
   "outputs": [],
   "source": [
    "Doc1 = [\"\"\"With the Union cabinet approving the amendments to the Motor Vehicles Act, 2016, those caught for drunken driving will \n",
    "have to have really deep pockets, as the fine payable in court has been enhanced to Rs 10,000 for first-time offenders.\"\"\" ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe00d7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "Doc2 = [\"\"\"Natural language processing (NLP) is an area of computer science and artificial intelligence concerned with the\n",
    "interactions between computers and human (natural) languages, in particular how to program computers to process and analyze\n",
    "large amounts of natural language data.\"\"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb33de70",
   "metadata": {},
   "outputs": [],
   "source": [
    "Doc3 = [\"\"\"He points out that public transport is very good in Mumbai and New Delhi, where there is a good network of suburban\n",
    "and metro rail systems.\"\"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "347b5ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Doc4 = [\"\"\"But the man behind the wickets at the other end was watching just as keenly. With an affirmative nod from Dhoni,\n",
    "India captain Rohit Sharma promptly asked for a review. Sure enough, the ball would have clipped the top of middle and leg.\"\"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "258372c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['With the Union cabinet approving the amendments to the Motor Vehicles Act, 2016, those caught for drunken driving will \\nhave to have really deep pockets, as the fine payable in court has been enhanced to Rs 10,000 for first-time offenders.',\n",
       " 'Natural language processing (NLP) is an area of computer science and artificial intelligence concerned with the\\ninteractions between computers and human (natural) languages, in particular how to program computers to process and analyze\\nlarge amounts of natural language data.',\n",
       " 'He points out that public transport is very good in Mumbai and New Delhi, where there is a good network of suburban\\nand metro rail systems.',\n",
       " 'But the man behind the wickets at the other end was watching just as keenly. With an affirmative nod from Dhoni,\\nIndia captain Rohit Sharma promptly asked for a review. Sure enough, the ball would have clipped the top of middle and leg.']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fin= Doc1+Doc2+Doc3+Doc4\n",
    "fin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b08b281c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "import nltk\n",
    "import itertools\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import scipy\n",
    "from scipy import spatial\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "import re\n",
    "tokenizer = ToktokTokenizer()\n",
    "stopword_list = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "25c6e8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As mentioned earlier, we are going to use the word embeddings to solve\n",
    "# this problem. Download word2vec from the below link:\n",
    "# https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "38e397b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the model\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('./GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38ef22f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(model.key_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "85880cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we build the information retrieval system:\n",
    "\n",
    "#Preprocessing\n",
    "def remove_stopwords(text, is_lower_case=False):\n",
    "    pattern = r'[^a-zA-z0-9\\s]'\n",
    "    text = re.sub(pattern, '', ''.join(text))\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    if is_lower_case:\n",
    "        filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
    "    else:\n",
    "        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n",
    "    filtered_text = ' '.join(filtered_tokens)\n",
    "    return filtered_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "76c9a503",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get the embedding vector for n dimension, we have used \"300\"\n",
    "def get_embedding(word):\n",
    "    if word in list(model.key_to_index):\n",
    "        return model[word]\n",
    "    else:\n",
    "        return np.zeros(300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "eb8f8933",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For every document, we will get a lot of vectors based on the number of\n",
    "# words present. We need to calculate the average vector for the document\n",
    "# through taking a mean of all the word vectors.\n",
    "\n",
    "\n",
    "# Getting average vector for each document\n",
    "out_dict = {}\n",
    "for sen in fin:\n",
    "    average_vector = (np.mean(np.array([get_embedding(x) for x in nltk.word_tokenize(remove_stopwords(sen))]), axis=0))\n",
    "    dict = { sen : (average_vector) }\n",
    "    out_dict.update(dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "05d34ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the similarity between the query vector and document vector\n",
    "\n",
    "def get_sim(query_embedding, average_vector_doc):\n",
    "    sim = [(1 - scipy.spatial.distance.cosine(query_embedding,\n",
    "    average_vector_doc))]\n",
    "    return sim\n",
    "\n",
    "# Rank all the documents based on the similarity to get relevant docs\n",
    "def Ranked_documents(query):\n",
    "    query_words = (np.mean(np.array([get_embedding(x) for x in nltk.word_tokenize(query.lower())],dtype=float), axis=0))\n",
    "    rank = []\n",
    "    for k,v in out_dict.items():\n",
    "        rank.append((k, get_sim(query_words, v)))\n",
    "    rank = sorted(rank,key=lambda t: t[1], reverse=True)\n",
    "    print('Ranked Documents :')\n",
    "    return rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4c837139",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranked Documents :\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('But the man behind the wickets at the other end was watching just as keenly. With an affirmative nod from Dhoni,\\nIndia captain Rohit Sharma promptly asked for a review. Sure enough, the ball would have clipped the top of middle and leg.',\n",
       "  [0.44954328830341783]),\n",
       " ('He points out that public transport is very good in Mumbai and New Delhi, where there is a good network of suburban\\nand metro rail systems.',\n",
       "  [0.23973446930269127]),\n",
       " ('With the Union cabinet approving the amendments to the Motor Vehicles Act, 2016, those caught for drunken driving will \\nhave to have really deep pockets, as the fine payable in court has been enhanced to Rs 10,000 for first-time offenders.',\n",
       "  [0.1832371201201335]),\n",
       " ('Natural language processing (NLP) is an area of computer science and artificial intelligence concerned with the\\ninteractions between computers and human (natural) languages, in particular how to program computers to process and analyze\\nlarge amounts of natural language data.',\n",
       "  [0.17995061678671642])]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let’s see how the information retrieval system we built is working with a\n",
    "# couple of examples.\n",
    "\n",
    "# Call the IR function with a query\n",
    "Ranked_documents(\"cricket\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ffe12a0",
   "metadata": {},
   "outputs": [],
   "source": [
    " #If you see, doc4 (on top in result), this will be most relevant for the\n",
    "# query “cricket” even though the word “cricket” is not even mentioned once\n",
    "# with the similarity of 0.449.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324dcbfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let’s see how the information retrieval system we built is working with a\n",
    "# couple of examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "273b599e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranked Documents :\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('With the Union cabinet approving the amendments to the Motor Vehicles Act, 2016, those caught for drunken driving will \\nhave to have really deep pockets, as the fine payable in court has been enhanced to Rs 10,000 for first-time offenders.',\n",
       "  [0.3594728772380067]),\n",
       " ('But the man behind the wickets at the other end was watching just as keenly. With an affirmative nod from Dhoni,\\nIndia captain Rohit Sharma promptly asked for a review. Sure enough, the ball would have clipped the top of middle and leg.',\n",
       "  [0.19042557661139026]),\n",
       " ('He points out that public transport is very good in Mumbai and New Delhi, where there is a good network of suburban\\nand metro rail systems.',\n",
       "  [0.1706653724240128]),\n",
       " ('Natural language processing (NLP) is an area of computer science and artificial intelligence concerned with the\\ninteractions between computers and human (natural) languages, in particular how to program computers to process and analyze\\nlarge amounts of natural language data.',\n",
       "  [0.08872308406410134])]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ranked_documents(\"driving\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1401e82f",
   "metadata": {},
   "source": [
    "We can use the same approach and scale it up for as many documents as possible. For more accuracy, we can build our own embeddings,  the one we are using is generalized.\n",
    "\n",
    "This is the fundamental approach that can be used for many applications like the following:\n",
    "\n",
    "• Search engines\\\n",
    "• Document retrieval\\\n",
    "• Passage retrieval\\\n",
    "• Question and answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430a296f",
   "metadata": {},
   "source": [
    "# Classifying Text with Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "eb8b5668",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "# from nltk import *\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sys, os, re, csv, codecs, numpy as np, pandas as pd\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
    "from tensorflow.keras.layers import Bidirectional, GlobalMaxPool1D, Conv1D, SimpleRNN\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from tensorflow.keras.layers import Dense, Input, Flatten, Dropout, BatchNormalization\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from tensorflow.keras.models import Sequential\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.layers.recurrent import SimpleRNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "575b0005",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c2c8e5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a text classification model using CNN, RNN, and LSTM.\n",
    "# Email classification (spam or ham)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ef31b876",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The approach and NLP pipeline would remain the same as discussed\n",
    "# earlier. The only change would be that instead of using machine learning\n",
    "# algorithms, we would be building models using deep learning algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "edec9571",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5567</th>\n",
       "      <td>spam</td>\n",
       "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5568</th>\n",
       "      <td>ham</td>\n",
       "      <td>Will Ì_ b going to esplanade fr home?</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5569</th>\n",
       "      <td>ham</td>\n",
       "      <td>Pity, * was in mood for that. So...any other s...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5570</th>\n",
       "      <td>ham</td>\n",
       "      <td>The guy did some bitching but I acted like i'd...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5571</th>\n",
       "      <td>ham</td>\n",
       "      <td>Rofl. Its true to its name</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5572 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        v1                                                 v2 Unnamed: 2  \\\n",
       "0      ham  Go until jurong point, crazy.. Available only ...        NaN   \n",
       "1      ham                      Ok lar... Joking wif u oni...        NaN   \n",
       "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...        NaN   \n",
       "3      ham  U dun say so early hor... U c already then say...        NaN   \n",
       "4      ham  Nah I don't think he goes to usf, he lives aro...        NaN   \n",
       "...    ...                                                ...        ...   \n",
       "5567  spam  This is the 2nd time we have tried 2 contact u...        NaN   \n",
       "5568   ham              Will Ì_ b going to esplanade fr home?        NaN   \n",
       "5569   ham  Pity, * was in mood for that. So...any other s...        NaN   \n",
       "5570   ham  The guy did some bitching but I acted like i'd...        NaN   \n",
       "5571   ham                         Rofl. Its true to its name        NaN   \n",
       "\n",
       "     Unnamed: 3 Unnamed: 4  \n",
       "0           NaN        NaN  \n",
       "1           NaN        NaN  \n",
       "2           NaN        NaN  \n",
       "3           NaN        NaN  \n",
       "4           NaN        NaN  \n",
       "...         ...        ...  \n",
       "5567        NaN        NaN  \n",
       "5568        NaN        NaN  \n",
       "5569        NaN        NaN  \n",
       "5570        NaN        NaN  \n",
       "5571        NaN        NaN  \n",
       "\n",
       "[5572 rows x 5 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read file\n",
    "file_content = pd.read_csv('spam.csv', encoding = \"ISO-8859-1\")\n",
    "#check sample content in the email\n",
    "file_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "cd09466e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ok lar... Joking wif u oni...'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_content['v2'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d202653d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Target</th>\n",
       "      <th>Email</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go jurong point, crazy.. Available bugis n gre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry 2 wkly comp win FA Cup final tkts 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say early hor... U c already say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I think goes usf, lives around though</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Target                                              Email\n",
       "0    ham  Go jurong point, crazy.. Available bugis n gre...\n",
       "1    ham                      Ok lar... Joking wif u oni...\n",
       "2   spam  Free entry 2 wkly comp win FA Cup final tkts 2...\n",
       "3    ham          U dun say early hor... U c already say...\n",
       "4    ham          Nah I think goes usf, lives around though"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove stop words\n",
    "stop = stopwords.words('english') \n",
    "file_content['v2'] = file_content['v2'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "# Delete unwanted columns\n",
    "Email_Data = file_content[['v1', 'v2']]\n",
    "# Rename column names\n",
    "Email_Data = Email_Data.rename(columns={\"v1\":\"Target\", \"v2\":\"Email\"})\n",
    "Email_Data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "70a98710",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    go jurong point crazy available bugis n great ...\n",
       "1                              ok lar joking wif u oni\n",
       "2    free entry 2 wkly comp win fa cup final tkts 2...\n",
       "3                  u dun say early hor u c already say\n",
       "4             nah i think goes usf lives around though\n",
       "Name: Email, dtype: object"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Delete punctuations, convert text in lower case and delete the double space\n",
    "\n",
    "Email_Data['Email'] = Email_Data['Email'].apply(lambda x:re.sub('[!@#$:).;,?&]', '', x.lower()))\n",
    "Email_Data['Email'] = Email_Data['Email'].apply(lambda x:re.sub(' ', ' ', x))\n",
    "Email_Data['Email'].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "697e66ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['ham'],\n",
       "       ['ham'],\n",
       "       ['spam'],\n",
       "       ...,\n",
       "       ['ham'],\n",
       "       ['ham'],\n",
       "       ['ham']], dtype=object)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Separating text(input) and target classes\n",
    "list_sentences_rawdata = Email_Data[\"Email\"].fillna(\"_na_\").values\n",
    "list_classes = [\"Target\"]\n",
    "target = Email_Data[list_classes].values\n",
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "52d583fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Email</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>go jurong point crazy available bugis n great ...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ok lar joking wif u oni</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>free entry 2 wkly comp win fa cup final tkts 2...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>u dun say early hor u c already say</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>nah i think goes usf lives around though</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5567</th>\n",
       "      <td>this 2nd time tried 2 contact u u å£750 pound ...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5568</th>\n",
       "      <td>will ì_ b going esplanade fr home</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5569</th>\n",
       "      <td>pity * mood that soany suggestions</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5570</th>\n",
       "      <td>the guy bitching i acted like i'd interested b...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5571</th>\n",
       "      <td>rofl its true name</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5572 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Email Target\n",
       "0     go jurong point crazy available bugis n great ...    ham\n",
       "1                               ok lar joking wif u oni    ham\n",
       "2     free entry 2 wkly comp win fa cup final tkts 2...   spam\n",
       "3                   u dun say early hor u c already say    ham\n",
       "4              nah i think goes usf lives around though    ham\n",
       "...                                                 ...    ...\n",
       "5567  this 2nd time tried 2 contact u u å£750 pound ...   spam\n",
       "5568                  will ì_ b going esplanade fr home    ham\n",
       "5569                 pity * mood that soany suggestions    ham\n",
       "5570  the guy bitching i acted like i'd interested b...    ham\n",
       "5571                                 rofl its true name    ham\n",
       "\n",
       "[5572 rows x 2 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "To_Process=Email_Data[['Email', 'Target']]\n",
    "To_Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7ac1f345",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train and test split with 80:20 ratio\n",
    "train, test = train_test_split(To_Process, test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "cea2a6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the sequence lengths, max number of words and embedding dimensions\n",
    "# Sequence length of each sentence. If more, truncate. If less, pad with zeros\n",
    "MAX_SEQUENCE_LENGTH = 300\n",
    "# Top 20000 frequently occurring words\n",
    "MAX_NB_WORDS = 20000\n",
    "\n",
    "\n",
    "# Get the frequently occurring words\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(train.Email)\n",
    "train_sequences = tokenizer.texts_to_sequences(train.Email)\n",
    "test_sequences = tokenizer.texts_to_sequences(test.Email)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9fa072a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "282af0dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8463 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "# dictionary containing words and their index\n",
    "word_index = tokenizer.word_index\n",
    "# print(tokenizer.word_index)\n",
    "# total words in the corpus\n",
    "print('Found %s unique tokens.' % len(word_index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "c2fbb0db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4457, 300)\n",
      "(1115, 300)\n"
     ]
    }
   ],
   "source": [
    "# get only the top frequent words on train\n",
    "train_data = pad_sequences(train_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "# get only the top frequent words on test\n",
    "test_data = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print(train_data.shape)\n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "ce92f671",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ham' 'spam']\n",
      "(array([0, 1]), array([3848,  609], dtype=int64))\n",
      "(array([0, 1]), array([977, 138], dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "train_labels = train['Target']\n",
    "test_labels = test['Target']\n",
    "\n",
    "# converts the character array to numeric array. Assigns levels to unique labels.\n",
    "le = LabelEncoder()\n",
    "le.fit(train_labels)\n",
    "train_labels = le.transform(train_labels)\n",
    "test_labels = le.transform(test_labels)\n",
    "print(le.classes_)\n",
    "print(np.unique(train_labels, return_counts=True))\n",
    "print(np.unique(test_labels, return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "3168ab59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (4457, 300)\n",
      "Shape of label tensor: (4457, 2)\n",
      "Shape of label tensor: (1115, 2)\n"
     ]
    }
   ],
   "source": [
    "# changing data types\n",
    "labels_train = to_categorical(np.asarray(train_labels))\n",
    "labels_test = to_categorical(np.asarray(test_labels))\n",
    "print('Shape of data tensor:', train_data.shape)\n",
    "print('Shape of label tensor:', labels_train.shape)\n",
    "print('Shape of label tensor:', labels_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "1de0a2f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       ...,\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "e9b95b27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_DIM = 100\n",
    "print(MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "d0232ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model building and predicting\n",
    "\n",
    "# define a single hidden layer with 128 memory units. The\n",
    "# network uses a dropout with a probability of 0.5. The output layer is a\n",
    "# dense layer using the softmax activation function to output a probability\n",
    "# prediction.\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Conv1D(128, 5, activation='relu'))\n",
    "model.add(MaxPooling1D(5))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv1D(128, 5, activation='relu'))\n",
    "model.add(MaxPooling1D(5))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(2, activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "38560d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',optimizer='rmsprop', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "c53c6a41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "70/70 [==============================] - 8s 91ms/step - loss: 0.3830 - acc: 0.8499 - val_loss: 0.4355 - val_acc: 0.8762\n",
      "Epoch 2/5\n",
      "70/70 [==============================] - 6s 87ms/step - loss: 0.1424 - acc: 0.9545 - val_loss: 0.3846 - val_acc: 0.8762\n",
      "Epoch 3/5\n",
      "70/70 [==============================] - 6s 87ms/step - loss: 0.0718 - acc: 0.9796 - val_loss: 0.4554 - val_acc: 0.8762\n",
      "Epoch 4/5\n",
      "70/70 [==============================] - 6s 86ms/step - loss: 0.0566 - acc: 0.9865 - val_loss: 0.4791 - val_acc: 0.8762\n",
      "Epoch 5/5\n",
      "70/70 [==============================] - 6s 86ms/step - loss: 0.0373 - acc: 0.9906 - val_loss: 0.4582 - val_acc: 0.8897\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x16d6d08ab80>"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_data, labels_train, batch_size=64, epochs=5, validation_data=(test_data, labels_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "55056cbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5217743 , 0.47822574],\n",
       "       [0.52167493, 0.4783251 ],\n",
       "       [0.52386904, 0.476131  ],\n",
       "       ...,\n",
       "       [0.5217612 , 0.4782388 ],\n",
       "       [0.5330771 , 0.46692288],\n",
       "       [0.5264083 , 0.47359166]], dtype=float32)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#predictions on test data\n",
    "predicted=model.predict(test_data)\n",
    "predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "f586d2cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: [0.936721 1.      ]\n",
      "recall: [1.         0.52173913]\n",
      "fscore: [0.96732673 0.68571429]\n",
      "support: [977 138]\n",
      "############################\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      1.00      0.97       977\n",
      "           1       1.00      0.52      0.69       138\n",
      "\n",
      "   micro avg       0.94      0.94      0.94      1115\n",
      "   macro avg       0.97      0.76      0.83      1115\n",
      "weighted avg       0.94      0.94      0.93      1115\n",
      " samples avg       0.94      0.94      0.94      1115\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#model evaluation\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "precision, recall, fscore, support = score(labels_test, predicted.round())\n",
    "print('precision: {}'.format(precision))\n",
    "print('recall: {}'.format(recall))\n",
    "print('fscore: {}'.format(fscore))\n",
    "print('support: {}'.format(support))\n",
    "print(\"############################\")\n",
    "print(classification_report(labels_test, predicted.round()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "b851d781",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN model\n",
    "from tensorflow.keras.layers import SimpleRNN\n",
    "model = Sequential()\n",
    "model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH))\n",
    "model.add(SimpleRNN(2, input_shape=(None,1)))\n",
    "model.add(Dense(2,activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "5314f43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss = 'binary_crossentropy', optimizer='adam', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "243cf7b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "279/279 [==============================] - 242s 861ms/step - loss: 0.5735 - accuracy: 0.8842 - val_loss: 0.4748 - val_accuracy: 0.9094\n",
      "Epoch 2/5\n",
      "279/279 [==============================] - 271s 973ms/step - loss: 0.3604 - accuracy: 0.9522 - val_loss: 0.3589 - val_accuracy: 0.9247\n",
      "Epoch 3/5\n",
      "279/279 [==============================] - 245s 878ms/step - loss: 0.2205 - accuracy: 0.9778 - val_loss: 0.2989 - val_accuracy: 0.9229\n",
      "Epoch 4/5\n",
      "279/279 [==============================] - 240s 859ms/step - loss: 0.1461 - accuracy: 0.9870 - val_loss: 0.2661 - val_accuracy: 0.9265\n",
      "Epoch 5/5\n",
      "279/279 [==============================] - 253s 906ms/step - loss: 0.1026 - accuracy: 0.9919 - val_loss: 0.2577 - val_accuracy: 0.9220\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x16d73f57850>"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_data, labels_train, batch_size=16, epochs=5, validation_data=(test_data, labels_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "7bcabf94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.990229  , 0.00977101],\n",
       "       [0.9979056 , 0.0020944 ],\n",
       "       [0.89674115, 0.10325881],\n",
       "       ...,\n",
       "       [0.9576678 , 0.04233219],\n",
       "       [0.9935895 , 0.00641052],\n",
       "       [0.9971961 , 0.00280388]], dtype=float32)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prediction on test data\n",
    "predicted_Srnn=model.predict(test_data)\n",
    "predicted_Srnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "128afcf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: [0.94322709 0.72972973]\n",
      "recall: [0.96929376 0.58695652]\n",
      "fscore: [0.95608279 0.65060241]\n",
      "support: [977 138]\n",
      "############################\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.97      0.96       977\n",
      "           1       0.73      0.59      0.65       138\n",
      "\n",
      "   micro avg       0.92      0.92      0.92      1115\n",
      "   macro avg       0.84      0.78      0.80      1115\n",
      "weighted avg       0.92      0.92      0.92      1115\n",
      " samples avg       0.92      0.92      0.92      1115\n",
      "\n"
     ]
    }
   ],
   "source": [
    "precision, recall, fscore, support = score(labels_test,predicted_Srnn.round())\n",
    "print('precision: {}'.format(precision))\n",
    "print('recall: {}'.format(recall))\n",
    "print('fscore: {}'.format(fscore))\n",
    "print('support: {}'.format(support))\n",
    "print(\"############################\")\n",
    "print(classification_report(labels_test, predicted_Srnn.round()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea68b0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074fcd02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import LSTM\n",
    "model = Sequential()\n",
    "model.add(Embedding(MAX_NB_WORDS,EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH))\n",
    "model.add(LSTM(output_dim=16, activation='relu', inner_activation='hard_sigmoid',return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Flatten())\n",
    "model.add(Dense(2,activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a517515e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss = 'binary_crossentropy',optimizer='adam',metrics = ['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7304edb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_data, labels_train,nbatch_size=16,epochs=5, validation_data=(test_data, labels_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd48e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prediction on text data\n",
    "predicted_lstm=model.predict(test_data)\n",
    "predicted_lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af66edc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision, recall, fscore, support = score(labels_test,\n",
    "predicted_lstm.round())\n",
    "print('precision: {}'.format(precision))\n",
    "print('recall: {}'.format(recall))\n",
    "print('fscore: {}'.format(fscore))\n",
    "print('support: {}'.format(support))\n",
    "print(\"############################\")\n",
    "print(sklearn.metrics.classification_report(labels_test,\n",
    "predicted_lstm.round()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "7b651044",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Bidirectional LSTM\n",
    "\n",
    "# As we know, LSTM preserves information from inputs using the\n",
    "# hidden state. In bidirectional LSTMs, inputs are fed in two ways: one\n",
    "# from previous to future and the other going backward from future to\n",
    "# past, helping in learning future representation as well. Bidirectional\n",
    "# LSTMs are known for producing very good results as they are capable of\n",
    "# understanding the context better.\n",
    "\n",
    "#  Bidirectional LSTM must outperform  the rest of the algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "d19dc5d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import LSTM\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH))\n",
    "model.add(Bidirectional(LSTM(16, return_sequences=True, dropout=0.1, recurrent_dropout=0.1)))\n",
    "model.add(Conv1D(16, kernel_size = 3, padding = \"valid\", kernel_initializer = \"glorot_uniform\"))\n",
    "model.add(GlobalMaxPool1D())\n",
    "model.add(Dense(50, activation=\"relu\"))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(2,activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "633ce65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss = 'binary_crossentropy', optimizer='adam',metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "4f8f0704",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.fit(train_data, labels_train, batch_size = 32, epochs =1, validation_data=(test_data, labels_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931277e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction on test data\n",
    "predicted_blstm=model.predict(test_data)\n",
    "predicted_blstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea14f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model evaluation\n",
    "from sklearn.metrics import precision_recall_fscore_support as\n",
    "score\n",
    "precision, recall, fscore, support = score(labels_test,\n",
    "predicted_blstm.round())\n",
    "print('precision: {}'.format(precision))\n",
    "print('recall: {}'.format(recall))\n",
    "print('fscore: {}'.format(fscore))\n",
    "print('support: {}'.format(support))\n",
    "print(\"############################\")\n",
    "print(classification_report(labels_test, predicted_blstm.round()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23299882",
   "metadata": {},
   "source": [
    "# Next Word Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69708ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
