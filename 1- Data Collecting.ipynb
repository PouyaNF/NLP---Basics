{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be3ffbe5",
   "metadata": {},
   "source": [
    "# Collect text data using Twitter APIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9005f726",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries\n",
    "import numpy as np\n",
    "import tweepy\n",
    "import json\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a2e3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# credentials\n",
    "consumer_key = \"adjbiejfaaoeh\"\n",
    "consumer_secret = \"had73haf78af\"\n",
    "access_token = \"jnsfby5u4yuawhafjeh\"\n",
    "access_token_secret = \"jhdfgay768476r\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4ff6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calling API\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tweepy.API(auth)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c75eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Provide the query you want to pull the data. For example,\n",
    "#pulling data for the mobile phone ABC\n",
    "query =\"ABC\"\n",
    "# Fetching tweets\n",
    "Tweets = api.search_tweets(query, count = 10,lang='en',\n",
    "exclude='retweets',tweet_mode='extended')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ec306b",
   "metadata": {},
   "source": [
    "The query above will pull the top 10 tweets when the product ABC is\n",
    "searched. The API will pull English tweets since the language given is ‘en’\n",
    "and it will exclude retweets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1823c2c",
   "metadata": {},
   "source": [
    "# Collect text data using PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708d31fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "from PyPDF2 import PdfFileReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a8d4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a pdf file object\n",
    "pdf = open(\"tgr.pdf\",\"rb\")\n",
    "#creating pdf reader object\n",
    "pdf_reader = PyPDF2.PdfFileReader(pdf)  #  doesn’t work for scanned PDFs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d92ce22",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking number of pages in a pdf file\n",
    "print(pdf_reader.numPages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14cedf7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a page object\n",
    "page = pdf_reader.getPage(9)\n",
    "#extracting text from the page\n",
    "print(page.extractText())\n",
    "#closing the pdf file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c48a942",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43a5891",
   "metadata": {},
   "source": [
    "# Collect text data using Word file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3674d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from docx import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69fd94c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a word file object\n",
    "doc = open(\"sample.docx\",\"rb\")\n",
    "#creating word reader object\n",
    "document = Document(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a779751",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an empty string and call this document. This document\n",
    "#variable store each paragraph in the Word document.We then\n",
    "#create a for loop that goes through each paragraph in the Word\n",
    "#document and appends the paragraph.\n",
    "docu=\"\"\n",
    "for para in document.paragraphs:\n",
    "    docu += para.text\n",
    "    \n",
    "print(docu)\n",
    "#to see the output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f9d0fc",
   "metadata": {},
   "source": [
    "# Collecting Data from JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c950d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c32ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#json from \"https://quotes.rest/qod.json\"\n",
    "r = requests.get(\"https://quotes.rest/qod.json\")\n",
    "res = r.json()\n",
    "print(json.dumps(res, indent = 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff242ec",
   "metadata": {},
   "source": [
    "# Collecting Data from HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b947fad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request as urllib2\n",
    "from bs4 import BeautifulSoup # Beautiful Soup is a Python library for pulling data out of HTML and XML files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c818c4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch the HTML file\n",
    "url = 'https://en.wikipedia.org/wiki/Natural_language_processing'\n",
    "response = urllib2.urlopen(url)\n",
    "html_doc = response.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d497b724",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the HTML file\n",
    "soup = BeautifulSoup(html_doc, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1baebe57",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Formating the parsed html file\n",
    "strhtm = soup.prettify()\n",
    "len(strhtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c989b335",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print few lines\n",
    "print (strhtm[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6296be6",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Extracting tag value\n",
    "print(soup.title)\n",
    "print(soup.title.string)\n",
    "print(soup.a.string)\n",
    "print(soup.b.string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd73ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.head.title  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14dae6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.body.b.text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6debfca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting all instances of a particular tag\n",
    "\n",
    "for x in soup.find_all('a') : print(x.string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c642e23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting all text of a particular tag\n",
    "for x in soup.find_all('p'): print(x.text)  # using the ‘p’ tag extracted most of the text present in the page"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b82b32",
   "metadata": {},
   "source": [
    "# Parsing Text Using Regularb Expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86b0505",
   "metadata": {},
   "source": [
    "__Basic flags__:\n",
    "\n",
    "re.I: This flag is used for ignoring casing.\\\n",
    "re.L: This flag is used to find a local dependent.\\\n",
    "re.M: This flag is useful if you want to find patterns throughout multiple lines.\\\n",
    "re.S: This flag is used to find dot matches.\\\n",
    "re.U: This flag is used to work for Unicode data.\\\n",
    "re.X: This flag is used for writing regex in a more readable format.\n",
    "\n",
    "__functionality__:\n",
    "\n",
    "Regex: [ab]                # Find the single occurrence of character a and b\\\n",
    "Regex: [^ab]              # Find characters except for a and b\\\n",
    "Regex: [a-z]              # Find all the characters a to z\\\n",
    "Regex: [^a-z]  # Find a range except to z\\\n",
    "Regex:         # Any single character:\n",
    "Regex: \\s      # Any whitespace character\\\n",
    "Regex: \\S      # Any non whitespace character\\\n",
    "Regex: \\d      # Any digit\\\n",
    "Regex: \\D      # Any non digit\\\n",
    "Regex: \\W      # Any non-words\\\n",
    "Regex: \\w      #  Any words\\\n",
    "Regex: (a|b)   # Either match a or b\\\n",
    "Regex: a? ; ?  # Matches zero or one occurrence but not more than one occurrence\\\n",
    "Regex: a* ; *  # The occurrence of a is zero times or more than that\\\n",
    "Regex: a+ ; +  # The occurrence of a is one time or more than that\\\n",
    "Regex: a{3}    # Exactly match three occurrences of a\\\n",
    "Regex: a{3,}   # Match simultaneous occurrences of a with 3 or more than 3\\\n",
    "Regex: a{3,6}  # Match simultaneous occurrences of a between 3 to 6\\\n",
    "Regex: ^       # Starting of the string\\\n",
    "Regex: $       # Ending of the string\\\n",
    "Regex: \\b      # Match word boundary\\\n",
    "Regex: \\B      # Non-word boundary\\\n",
    "re.match()     # This checks for a match of the string only at the beginning of the string. So, if it finds the pattern at the  beginning of the input string, then it returns the matched pattern; otherwise; it returns a noun.\\\n",
    "re.search()    # This checks for a match of the string anywhere in the string. It finds all the occurrences of the pattern in the given input string or data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88f4cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracing email IDs\n",
    "doc = \"For more details please mail us at: xyz@abc.com, pqr@mno.com\"\n",
    "addresses = re.findall(r'[\\w\\.-]+@[\\w\\.-]+', doc)\n",
    "for address in addresses:\n",
    "    print(address)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77228cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing email IDs\n",
    "doc = \"For more details please mail us at xyz@abc.com\"\n",
    "new_email_address = re.sub(r'([\\w\\.-]+)@([\\w\\.-]+)', r'pqr@mno.com', doc)\n",
    "print(new_email_address)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0305d48a",
   "metadata": {},
   "outputs": [],
   "source": [
    " #Extract data from the ebook and perform regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b92b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import library\n",
    "import re\n",
    "import requests\n",
    "#url you want to extract\n",
    "url = 'https://www.gutenberg.org/files/2638/2638-0.txt'\n",
    "\n",
    "#function to extract\n",
    "def get_book(url):\n",
    "    # Sends a http request to get the text from project Gutenberg\n",
    "    raw = requests.get(url).text\n",
    "    # Discards the metadata from the beginning of the book\n",
    "    try : \n",
    "        start = re.search(r\"\\*\\*\\* START OF THIS PROJECT GUTENBERG EBOOK .* \\*\\*\\*\",raw ).end()\n",
    "    except AttributeError:\n",
    "        start = re.search(r\"\\*\\*\\* START OF THIS PROJECT GUTENBERG EBOOK .* \\*\\*\\*\",raw )\n",
    "    # Discards the metadata from the end of the book\n",
    "    stop = re.search(r\"II\", raw).start()\n",
    "    # Keeps the relevant text\n",
    "    text = raw[start:stop]\n",
    "    return text\n",
    "                      \n",
    "# processing\n",
    "def preprocess(sentence):\n",
    "    return re.sub('[^A-Za-z0-9.]+' , ' ', sentence).lower()\n",
    "                      \n",
    "#calling the above function\n",
    "book = get_book(url)\n",
    "processed_book = preprocess(book)\n",
    "print(processed_book)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d310876a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform some exploratory data analysis on this data using regex\n",
    "len(re.findall(r'the', processed_book))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c279ff2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replace \"i\" with \"I\"\n",
    "processed_book = re.sub(r'\\si\\s', \" I \", processed_book)\n",
    "print(processed_book)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09496aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#find all occurance of text in the format \"abc--xyz\"\n",
    "re.findall(r'[a-zA-Z0-9]*--[a-zA-Z0-9]*', book)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99334cc",
   "metadata": {},
   "source": [
    "# Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5115472",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "text = 'Natural language processing (NLP) is an interdisciplinary subfield of linguistics, computer science, and artificial intelligence'\n",
    "re.split('\\s' , text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a4fa26",
   "metadata": {},
   "source": [
    "# Handling Strings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fce85e5",
   "metadata": {},
   "source": [
    "s.find(t) index of first instance of string t inside s (-1 if not found)\\\n",
    "s.rfind(t) index of last instance of string t inside s (-1 if not found)\\\n",
    "s.index(t) like s.find(t) except it raises ValueError if not found\\\n",
    "s.rindex(t) like s.rfind(t) except it raises ValueError if not found\\\n",
    "s.join(text) combine the words of the text into a string using s as the glue\\\n",
    "s.split(t) split s into a list wherever a t is found (whitespace by default)\\\n",
    "s.splitlines() split s into a list of strings, one per line\\\n",
    "s.lower() a lowercased version of the string s\\\n",
    "s.upper() an uppercased version of the string s\\\n",
    "s.title() a titlecased version of the string s\\\n",
    "s.strip() a copy of s without leading or trailing whitespace\\\n",
    "s.replace(t, u) replace instances of t with u inside s\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ac6bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing content\n",
    "String_v1 = \"I am exploring NLP\"\n",
    "#To extract particular character or range of characters from string\n",
    "print(String_v1[0])\n",
    "print(String_v1[5:14])\n",
    "String_v2 = String_v1.replace(\"exploring\", \"learning\")\n",
    "print(String_v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151bdf85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenating two strings\n",
    "s1 = \"nlp\"\n",
    "s2 = \"machine learning\"\n",
    "s3 = s1+', '+s2\n",
    "s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8fe9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Searching for a substring in a string\n",
    "# Use the find function to fetch the starting index value of the substring in the whole string.\n",
    "s=\"I am learning NLP\"\n",
    "f= \"learn\"\n",
    "s.find(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccbbb9e0",
   "metadata": {},
   "source": [
    "# Scraping Text from the Web"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a394a43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You want to extract data from the web by scraping. Here we have taken the\n",
    "# example of the IMDB website for scraping top movies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0135ccba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "from pandas import Series, DataFrame\n",
    "from ipywidgets import FloatProgress\n",
    "from time import sleep\n",
    "from IPython.display import display\n",
    "import re\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4054d1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'http://www.imdb.com/chart/top?ref_=nv_mv_250_6'\n",
    "\n",
    "# Request the url and download the content using beautiful soup\n",
    "\n",
    "result = requests.get(url)\n",
    "c = result.content\n",
    "soup = BeautifulSoup(c, \"lxml\")\n",
    "\n",
    "# Understand the website page structure to extract the required information and\n",
    "# Use beautiful soup to extract and parse the data from HTML tags\n",
    "\n",
    "summary = soup.find('div', {'class': 'article'})\n",
    "\n",
    "# Create empty lists to append the extracted data.\n",
    "moviename = []\n",
    "cast = []\n",
    "description = []\n",
    "rating = []\n",
    "ratingoutof = []\n",
    "year = []\n",
    "genre = []\n",
    "movielength = []\n",
    "rot_audscore = []\n",
    "rot_avgrating = []\n",
    "rot_users = []\n",
    "\n",
    "# Extracting the required data from the html soup.\n",
    "rgx = re.compile('[%s]' % '()')\n",
    "f = FloatProgress(min=0, max=250)\n",
    "display(f)\n",
    "\n",
    "for row, i in zip(summary.find('table').findAll('tr'),\n",
    "                  range(len(summary.find('table').findAll('tr')))):\n",
    "    for sitem in row.findAll('span', {'class': 'secondaryInfo'}):\n",
    "        s = sitem.find(text=True)\n",
    "        year.append(rgx.sub('', s))\n",
    "    for ritem in row.findAll('td', {'class': 'ratingColumnimdbRating'}):\n",
    "        for iget in ritem.findAll('strong'):\n",
    "            rating.append(iget.find(text=True))\n",
    "            ratingoutof.append(iget.get('title').split(' ', 4)[3])\n",
    "    for item in row.findAll('td', {'class': 'titleColumn'}):\n",
    "        for href in item    .findAll('a', href=True):\n",
    "            moviename.append(href.find(text=True))\n",
    "            rurl = 'https://www.rottentomatoes.com/m/' + href.find(text=True)\n",
    "        try:\n",
    "            rresult = requests.get(rurl)\n",
    "        except requests.exceptions.ConnectionError:\n",
    "            status_code = \"Connection refused\"\n",
    "        rc = rresult.content\n",
    "        rsoup = BeautifulSoup(rc)\n",
    "        try:\n",
    "            rot_audscore.append(\n",
    "                rsoup.find('div', {'class': 'meter-value'}).find('span', {'class': 'superPageFontColor'}).text)\n",
    "            rot_avgrating.append(rsoup.find('div',\n",
    "                                            {'class': 'audience-info hidden-xs superPageFontColor'}).find(\n",
    "                'div').contents[2].strip())\n",
    "            rot_users.append(\n",
    "                rsoup.find('div', {'class': 'audience-info hidden-xs superPageFontColor'}).contents[3].contents[2].\n",
    "                strip())\n",
    "        except AttributeError:\n",
    "            rot_audscore.append(\"\")\n",
    "            rot_avgrating.append(\"\")\n",
    "            rot_users.append(\"\")\n",
    "        cast.append(href.get('title'))\n",
    "        imdb = \"http://www.imdb.com\" + href.get('href')\n",
    "        try:\n",
    "            iresult = requests.get(imdb)\n",
    "            ic = iresult.content\n",
    "            isoup = BeautifulSoup(ic)\n",
    "            description.append(isoup.find('div', {'class': 'summary_text'}).find(text=True).strip())\n",
    "            genre.append(isoup.find('span', {'class': 'itemprop'}).find(text=True))\n",
    "            movielength.append(isoup.find('time',\n",
    "                                          {'itemprop': 'duration'}).find(text=True).strip())\n",
    "        except requests.exceptions.ConnectionError:\n",
    "            description.append(\"\")\n",
    "            genre.append(\"\")\n",
    "            movielength.append(\"\")\n",
    "sleep(.1)\n",
    "f.value = i\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e3df2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert lists to data frame and you can\n",
    "# perform the analysis that meets the business\n",
    "# requirements\n",
    "\n",
    "# List to pandas series\n",
    "moviename = Series(moviename)\n",
    "cast = Series(cast)\n",
    "description = Series(description)\n",
    "rating = Series(rating)\n",
    "ratingoutof = Series(ratingoutof)\n",
    "year = Series(year)\n",
    "genre = Series(genre)\n",
    "movielength = Series(movielength)\n",
    "rot_audscore = Series(rot_audscore)\n",
    "rot_avgrating = Series(rot_avgrating)\n",
    "rot_users = Series(rot_users)\n",
    "\n",
    "# creating dataframe and doing analysis\n",
    "imdb_df = pd.concat([moviename,year,description,genre,\n",
    "movielength,cast,rating,ratingoutof,\n",
    "rot_audscore,rot_avgrating,rot_users],axis=1)\n",
    "imdb_df.columns = [ 'moviename','year','description','genre',\n",
    "'movielength','cast','imdb_rating',\n",
    "'imdb_ratingbasedon','tomatoes_audscore',\n",
    "'tomatoes_rating','tomatoes_ratingbasedon']\n",
    "imdb_df['rank'] = imdb_df.index + 1\n",
    "imdb_df.head(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
