{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92c8eb5e",
   "metadata": {},
   "source": [
    "# Extracting Noun Phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2c23ad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "john\n",
      "natural language processing\n"
     ]
    }
   ],
   "source": [
    "#Import libraries\n",
    "# import nltk\n",
    "from textblob import TextBlob\n",
    "#Extract noun\n",
    "blob = TextBlob(\"John is learning natural language processing\")\n",
    "for np in blob.noun_phrases:\n",
    "    print(np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7eb43832",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordList(['john', 'natural language processing'])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob.noun_phrases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b42909f",
   "metadata": {},
   "source": [
    "# Finding Similarity Between Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8305b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The simplest way to do this is by using cosine similarity from the sklearn library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "338c8ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = (\n",
    "\"I like NLP\",\n",
    "\"I am exploring NLP\",\n",
    "\"I am a beginner in NLP\",\n",
    "\"I want to learn NLP\",\n",
    "\"I like advanced NLP\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b20fa93e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 10)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Import libraries\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "#Compute tfidf : feature engineering\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(documents)\n",
    "tfidf_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ed0fa83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.17682765, 0.14284054, 0.13489366, 0.68374784]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#compute similarity for first sentence with rest of the sentences\n",
    "\n",
    "# If we clearly observe, the first sentence and last sentence have higher similarity compared to the rest of the sentences.\n",
    "cosine_similarity(tfidf_matrix[0:1],tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7a8d3b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.17682765, 1.        , 0.37765328, 0.09223325, 0.12090552]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(tfidf_matrix[1:2],tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc08b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phonetic matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc61a4a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fuzzy\n",
      "  Downloading Fuzzy-1.2.2.tar.gz (14 kB)\n",
      "Building wheels for collected packages: fuzzy\n",
      "  Building wheel for fuzzy (setup.py): started\n",
      "  Building wheel for fuzzy (setup.py): finished with status 'done'\n",
      "  Created wheel for fuzzy: filename=Fuzzy-1.2.2-cp39-cp39-win_amd64.whl size=38936 sha256=c26040dcb05f154a32ae1414f1444cf657ab6e10911068b0cfccd7bc2809ae16\n",
      "  Stored in directory: c:\\users\\pooya\\appdata\\local\\pip\\cache\\wheels\\cd\\72\\5c\\3baf339199d883de942158b8759f447b8ecf14e4ec522294b4\n",
      "Successfully built fuzzy\n",
      "Installing collected packages: fuzzy\n",
      "Successfully installed fuzzy-1.2.2\n"
     ]
    }
   ],
   "source": [
    "!pip install fuzzy\n",
    "import fuzzy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0960c274",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the Soundex function\n",
    "soundex = fuzzy.Soundex(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d07c4f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('N364', 'N364', 'L652')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soundex('natural')  , soundex('Natural') ,  soundex('Learning')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4c8718",
   "metadata": {},
   "source": [
    "# Tagging Part of Speech "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3cd3f6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"I love NLP and I will learn NLP in 2 month\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "76284a28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', 'PRP'),\n",
       " ('love', 'VBP'),\n",
       " ('NLP', 'NNP'),\n",
       " ('I', 'PRP'),\n",
       " ('learn', 'VBP'),\n",
       " ('NLP', 'RB'),\n",
       " ('2', 'CD'),\n",
       " ('month', 'NN')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing necessary packages and stopwords\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "stop_words = set(stopwords.words('english'))\n",
    "# Tokenize the text\n",
    "tokens = sent_tokenize(text)\n",
    "#Generate tagging for all the tokens using loop\n",
    "for i in tokens:\n",
    "    words = nltk.word_tokenize(i)\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "# POS-tagger.\n",
    "tags = nltk.pos_tag(words)\n",
    "tags"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704178e5",
   "metadata": {},
   "source": [
    "# Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ac36bd11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install svgling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5278c8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import ne_chunk\n",
    "from nltk import word_tokenize\n",
    "\n",
    "sent = \"John is studying at Stanford University in California\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eb3e9b34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['John', 'is', 'studying', 'at', 'Stanford', 'University', 'in', 'California']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eaa2903b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('John', 'NNP'),\n",
       " ('is', 'VBZ'),\n",
       " ('studying', 'VBG'),\n",
       " ('at', 'IN'),\n",
       " ('Stanford', 'NNP'),\n",
       " ('University', 'NNP'),\n",
       " ('in', 'IN'),\n",
       " ('California', 'NNP')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.pos_tag(word_tokenize(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9009b939",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg baseProfile=\"full\" height=\"168px\" preserveAspectRatio=\"xMidYMid meet\" style=\"font-family: times, serif; font-weight:normal; font-style: normal; font-size: 16px;\" version=\"1.1\" viewBox=\"0,0,520.0,168.0\" width=\"520px\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:ev=\"http://www.w3.org/2001/xml-events\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">S</text></svg><svg width=\"12.3077%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">PERSON</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">John</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"6.15385%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"7.69231%\" x=\"12.3077%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">is</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">VBZ</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"16.1538%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"15.3846%\" x=\"20%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">studying</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">VBG</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"27.6923%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"6.15385%\" x=\"35.3846%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">at</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">IN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"38.4615%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"33.8462%\" x=\"41.5385%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">ORGANIZATION</text></svg><svg width=\"45.4545%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">Stanford</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"22.7273%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"54.5455%\" x=\"45.4545%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">University</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"72.7273%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"58.4615%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"6.15385%\" x=\"75.3846%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">in</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">IN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"78.4615%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"18.4615%\" x=\"81.5385%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">GPE</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">California</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"90.7692%\" y1=\"1.2em\" y2=\"3em\" /></svg>"
      ],
      "text/plain": [
       "Tree('S', [Tree('PERSON', [('John', 'NNP')]), ('is', 'VBZ'), ('studying', 'VBG'), ('at', 'IN'), Tree('ORGANIZATION', [('Stanford', 'NNP'), ('University', 'NNP')]), ('in', 'IN'), Tree('GPE', [('California', 'NNP')])])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ne_chunk(nltk.pos_tag(word_tokenize(sent)), binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ee2ac49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using SpaCy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1470d3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.0/en_core_web_sm-2.2.0.tar.gz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "74e073e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "# Read/create a sentence\n",
    "doc = nlp(u'Apple is ready to launch new phone worth $10000 in\n",
    "New york time square ')\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)\n",
    "#output\n",
    "# Apple 0 5 ORG\n",
    "# 10000 42 47 MONEY\n",
    "# New york 51 59 GPE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5cf464",
   "metadata": {},
   "source": [
    "# Extracting Topics from Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e544f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify topics within documents using genism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "71230850",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install gensim\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "b5d14ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1 = \"I am learning NLP, it is very interesting and exciting. it includes machine learning and deep learning\"\n",
    "doc2 = \"My father is a data scientist and he is nlp expert\"\n",
    "doc3 = \"My sister has good exposure into android development\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "b1247e34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I am learning NLP, it is very interesting and exciting. it includes machine learning and deep learning',\n",
       " 'My father is a data scientist and he is nlp expert',\n",
       " 'My sister has good exposure into android development']"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_complete = [doc1, doc2, doc3]\n",
    "doc_complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "a77b171e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text preprocessing\n",
    "stop = set(stopwords.words('english'))\n",
    "exclude = set(string.punctuation)\n",
    "lemma = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "804de220",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(doc):\n",
    "    stop_free = ' '.join([i for i in doc.lower().split() if i not in stop])\n",
    "    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n",
    "    normalized = ' '.join(lemma.lemmatize(word) for word in punc_free.split())\n",
    "    return normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "10e13db2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['learning',\n",
       "  'nlp',\n",
       "  'interesting',\n",
       "  'exciting',\n",
       "  'includes',\n",
       "  'machine',\n",
       "  'learning',\n",
       "  'deep',\n",
       "  'learning'],\n",
       " ['father', 'data', 'scientist', 'nlp', 'expert'],\n",
       " ['sister', 'good', 'exposure', 'android', 'development']]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_clean = [clean(doc).split() for doc in doc_complete]\n",
    "doc_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea50c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing document term matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "fac056fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 'deep'),\n",
       " (1, 'exciting'),\n",
       " (2, 'includes'),\n",
       " (3, 'interesting'),\n",
       " (4, 'learning'),\n",
       " (5, 'machine'),\n",
       " (6, 'nlp'),\n",
       " (7, 'data'),\n",
       " (8, 'expert'),\n",
       " (9, 'father'),\n",
       " (10, 'scientist'),\n",
       " (11, 'android'),\n",
       " (12, 'development'),\n",
       " (13, 'exposure'),\n",
       " (14, 'good'),\n",
       " (15, 'sister')]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing gensim\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "# Creating the term dictionary of our corpus, where every unique term is assigned an index.\n",
    "dictionary = corpora.Dictionary(doc_clean)\n",
    "\n",
    "list(dictionary.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "7fecbb40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 3), (5, 1), (6, 1)],\n",
       " [(6, 1), (7, 1), (8, 1), (9, 1), (10, 1)],\n",
       " [(11, 1), (12, 1), (13, 1), (14, 1), (15, 1)]]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Converting a list of documents (corpus) into Document-Term Matrix using dictionary prepared above.\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]   # doc2bow : Convert document into the bag-of-words (BoW) format = list of (token_id, token_count) tuples.\n",
    "doc_term_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c818ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the LDA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "6147168d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '0.063*\"nlp\" + 0.063*\"father\" + 0.063*\"data\" + 0.063*\"scientist\" + 0.063*\"expert\" + 0.063*\"exposure\" + 0.063*\"development\" + 0.063*\"good\" + 0.063*\"android\" + 0.063*\"sister\"'), (1, '0.087*\"sister\" + 0.087*\"good\" + 0.087*\"development\" + 0.087*\"android\" + 0.087*\"exposure\" + 0.087*\"father\" + 0.087*\"scientist\" + 0.087*\"data\" + 0.087*\"expert\" + 0.087*\"nlp\"'), (2, '0.232*\"learning\" + 0.093*\"nlp\" + 0.093*\"deep\" + 0.093*\"includes\" + 0.093*\"interesting\" + 0.093*\"machine\" + 0.093*\"exciting\" + 0.023*\"scientist\" + 0.023*\"expert\" + 0.023*\"father\"')]\n"
     ]
    }
   ],
   "source": [
    "# Creating the object for LDA model using gensim library\n",
    "Lda = gensim.models.ldamodel.LdaModel\n",
    "# Running and Training LDA model on the document term matrix for 3 topics.\n",
    "ldamodel = Lda(doc_term_matrix, num_topics=3, id2word = dictionary, passes=50)\n",
    "# Results\n",
    "print(ldamodel.print_topics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d1e0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the weights associated with the topics from the sentence seem\n",
    "# almost similar. You can perform this on huge data to extract significant\n",
    "# topics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d27752",
   "metadata": {},
   "source": [
    "# Classifying Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "7644a1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spam - ham classification using machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "e83657db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "from textblob import TextBlob\n",
    "from nltk.stem import PorterStemmer\n",
    "from textblob import Word\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import sklearn.feature_extraction.text as text\n",
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "830e54ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read the data\n",
    "Email_Data = pd.read_csv(\"spam.csv\",encoding ='latin1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "7f58bb0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['v1', 'v2', 'Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4'], dtype='object')"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Data undestanding\n",
    "Email_Data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "b4e48d1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     v1                                                 v2 Unnamed: 2  \\\n",
       "0   ham  Go until jurong point, crazy.. Available only ...        NaN   \n",
       "1   ham                      Ok lar... Joking wif u oni...        NaN   \n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...        NaN   \n",
       "3   ham  U dun say so early hor... U c already then say...        NaN   \n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...        NaN   \n",
       "\n",
       "  Unnamed: 3 Unnamed: 4  \n",
       "0        NaN        NaN  \n",
       "1        NaN        NaN  \n",
       "2        NaN        NaN  \n",
       "3        NaN        NaN  \n",
       "4        NaN        NaN  "
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Email_Data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "2e788412",
   "metadata": {},
   "outputs": [],
   "source": [
    "Email_Data = Email_Data[['v1', 'v2']]\n",
    "Email_Data = Email_Data.rename(columns={\"v1\":\"Target\", \"v2\":\"Email\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "0520f0d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Target</th>\n",
       "      <th>Email</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Target                                              Email\n",
       "0    ham  Go until jurong point, crazy.. Available only ...\n",
       "1    ham                      Ok lar... Joking wif u oni...\n",
       "2   spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3    ham  U dun say so early hor... U c already then say...\n",
       "4    ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Email_Data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "8e503edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text processing and feature engineering\n",
    "\n",
    "#pre processing steps like lower case, stemming and lemmatization\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "7c10c91f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Target</th>\n",
       "      <th>Email</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>go until jurong point, crazy.. available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>ok lar... joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>free entry in 2 a wkly comp to win fa cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>u dun say so early hor... u c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>nah i don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Target                                              Email\n",
       "0    ham  go until jurong point, crazy.. available only ...\n",
       "1    ham                      ok lar... joking wif u oni...\n",
       "2   spam  free entry in 2 a wkly comp to win fa cup fina...\n",
       "3    ham  u dun say so early hor... u c already then say...\n",
       "4    ham  nah i don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Email_Data['Email'] = Email_Data['Email'].apply(lambda x:\" \".join(x.lower() for x in x.split()))\n",
    "Email_Data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "aaa9fc69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Target</th>\n",
       "      <th>Email</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>go jurong point, crazy.. available bugis n gre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>ok lar... joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>free entry 2 wkly comp win fa cup final tkts 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>u dun say early hor... u c already say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>nah think goes usf, lives around though</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Target                                              Email\n",
       "0    ham  go jurong point, crazy.. available bugis n gre...\n",
       "1    ham                      ok lar... joking wif u oni...\n",
       "2   spam  free entry 2 wkly comp win fa cup final tkts 2...\n",
       "3    ham          u dun say early hor... u c already say...\n",
       "4    ham            nah think goes usf, lives around though"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop = stopwords.words('english')\n",
    "Email_Data['Email'] = Email_Data['Email'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "Email_Data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "482ab24a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Target</th>\n",
       "      <th>Email</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>go jurong point, crazy.. avail bugi n great wo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>ok lar... joke wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>free entri 2 wkli comp win fa cup final tkt 21...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>u dun say earli hor... u c alreadi say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>nah think goe usf, live around though</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Target                                              Email\n",
       "0    ham  go jurong point, crazy.. avail bugi n great wo...\n",
       "1    ham                        ok lar... joke wif u oni...\n",
       "2   spam  free entri 2 wkli comp win fa cup final tkt 21...\n",
       "3    ham          u dun say earli hor... u c alreadi say...\n",
       "4    ham              nah think goe usf, live around though"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st = PorterStemmer()\n",
    "Email_Data['Email'] = Email_Data['Email'].apply(lambda x: \" \".join([st.stem(word) for word in x.split()]))\n",
    "Email_Data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "6da8ef9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Target</th>\n",
       "      <th>Email</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>go jurong point, crazy.. avail bugi n great wo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>ok lar... joke wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>free entri 2 wkli comp win fa cup final tkt 21...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>u dun say earli hor... u c alreadi say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>nah think goe usf, live around though</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Target                                              Email\n",
       "0    ham  go jurong point, crazy.. avail bugi n great wo...\n",
       "1    ham                        ok lar... joke wif u oni...\n",
       "2   spam  free entri 2 wkli comp win fa cup final tkt 21...\n",
       "3    ham          u dun say earli hor... u c alreadi say...\n",
       "4    ham              nah think goe usf, live around though"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Email_Data['Email'] = Email_Data['Email'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\n",
    "Email_Data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "8f48037b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting data into train and validation\n",
    "train_x, valid_x, train_y, valid_y = model_selection.train_test_split(Email_Data['Email'], Email_Data['Target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "c9a5b811",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 1, 0])"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TFIDF feature generation for a maximum of 5000 features\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "train_y = encoder.fit_transform(train_y)\n",
    "valid_y = encoder.fit_transform(valid_y)\n",
    "valid_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "a6db7c75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.52252739, 0.36420907, 0.35149393, ..., 0.32943488, 0.72333291,\n",
       "       0.69049945])"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000)\n",
    "tfidf_vect.fit(Email_Data['Email'])\n",
    "xtrain_tfidf = tfidf_vect.transform(train_x)\n",
    "xvalid_tfidf = tfidf_vect.transform(valid_x)\n",
    "xtrain_tfidf.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "794b6e72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4179x5000 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 38319 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtrain_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "c0063574",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9877961234745154\n"
     ]
    }
   ],
   "source": [
    "# Model training\n",
    "\n",
    "def train_model(classifier, feature_vector_train, label, feature_vector_valid, is_neural_net=False):\n",
    "    # fit the training dataset on the classifier\n",
    "    classifier.fit(feature_vector_train, label)\n",
    "    # predict the labels on validation dataset\n",
    "    predictions = classifier.predict(feature_vector_valid)\n",
    "    return metrics.accuracy_score(predictions, valid_y)\n",
    "\n",
    "\n",
    "# Naive Bayes trainig\n",
    "accuracy = train_model(naive_bayes.MultinomialNB(alpha=0.2), xtrain_tfidf, train_y, xvalid_tfidf)\n",
    "print (\"Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "b00cca45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9605168700646087\n"
     ]
    }
   ],
   "source": [
    "# Linear Classifier on Word Level TF IDF Vectors\n",
    "accuracy = train_model(linear_model.LogisticRegression(), xtrain_tfidf, train_y, xvalid_tfidf)\n",
    "print (\"Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b659a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes is giving better results than the linear classifier. We can try\n",
    "# many more classifiers and then choose the best one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e29aaa",
   "metadata": {},
   "source": [
    "# Carrying Out Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "ee1baa4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the sample data\n",
    "\n",
    "review = \"I like this phone. screen quality and camera clarity is really good.\"\n",
    "review2 = \"This tv is not good. Bad quality, no clarity, worst experience\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "272cc226",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment(polarity=0.7, subjectivity=0.6000000000000001)"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cleaning and preprocessing\n",
    "\n",
    "# Using a pretrained model from TextBlob to get the sentiment scores:\n",
    "from textblob import TextBlob\n",
    "#TextBlob has a pre trained sentiment prediction model\n",
    "blob = TextBlob(review)\n",
    "blob.sentiment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "9cdc3c38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment(polarity=-0.6833333333333332, subjectivity=0.7555555555555555)"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#now lets look at the sentiment of review2\n",
    "blob = TextBlob(review2)\n",
    "blob.sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70843dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a negative review, as the polarity is “-0.68.”\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b65716",
   "metadata": {},
   "source": [
    "# Disambiguating Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "df0a93f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pywsd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "ebf25310",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warming up PyWSD (takes ~10 secs)... took 2.7372796535491943 secs.\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem import PorterStemmer\n",
    "from itertools import chain\n",
    "from pywsd.lesk import simple_lesk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "6b94242b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context-1: I went to the bank to deposit my money\n",
      "Sense: Synset('depository_financial_institution.n.01')\n",
      "Definition :  a financial institution that accepts deposits and channels the money into lending activities\n"
     ]
    }
   ],
   "source": [
    "bank_sents = ['I went to the bank to deposit my money','The river bank was full of dead fishes']\n",
    "\n",
    "# calling the lesk function and printing results for both the sentences\n",
    "print (\"Context-1:\", bank_sents[0])\n",
    "answer = simple_lesk(bank_sents[0],'bank')\n",
    "print (\"Sense:\", answer)\n",
    "print (\"Definition : \", answer.definition())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "6164eb6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context-2: The river bank was full of dead fishes\n",
      "Sense: Synset('bank.n.01')\n",
      "Definition :  sloping land (especially the slope beside a body of water)\n"
     ]
    }
   ],
   "source": [
    "print (\"Context-2:\", bank_sents[1])\n",
    "answer = simple_lesk(bank_sents[1],'bank','n')\n",
    "print (\"Sense:\", answer)\n",
    "print (\"Definition : \", answer.definition())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9ca96b",
   "metadata": {},
   "source": [
    "# Converting Speech to Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "98c8f83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting speech to text is a very useful NLP technique.\n",
    "\n",
    "#!pip install SpeechRecognition\n",
    "#!pip install PyAudio\n",
    "import speech_recognition as sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "d65a637a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please say something\n",
      "Time over, thanks\n",
      "result2:\n",
      "{   'alternative': [   {   'confidence': 0.95467669,\n",
      "                           'transcript': 'speech recognition'}],\n",
      "    'final': True}\n",
      "I think you said: speech recognition\n"
     ]
    }
   ],
   "source": [
    "# whatever you say on the microphone (using recognize_google function) gets converted into text.\n",
    "r=sr.Recognizer()\n",
    "\n",
    "with sr.Microphone() as source:\n",
    "    print(\"Please say something\")\n",
    "    audio = r.listen(source)\n",
    "    print(\"Time over, thanks\")\n",
    "\n",
    "try:\n",
    "    print(\"I think you said: \" + r.recognize_google(audio));\n",
    "except:\n",
    "    pass;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "a67cf185",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please say something\n",
      "Time over, thanks\n",
      "result2:\n",
      "{   'alternative': [   {   'confidence': 0.89485103,\n",
      "                           'transcript': 'hallo wie geht es dir'},\n",
      "                       {'transcript': 'Hallo wie geht es dir'}],\n",
      "    'final': True}\n",
      "I think you said: hallo wie geht es dir\n"
     ]
    }
   ],
   "source": [
    "#code snippet\n",
    "r=sr.Recognizer()\n",
    "with sr.Microphone() as source:\n",
    "    print(\"Please say something\")\n",
    "    audio = r.listen(source)\n",
    "print(\"Time over, thanks\")\n",
    "try:\n",
    "    print(\"I think you said: \"+r.recognize_google(audio, language ='de-DE'));\n",
    "except sr.UnknownValueError:\n",
    "    print(\"Google Speech Recognition could not understand audio\")\n",
    "except sr.RequestError as e:\n",
    "    print(\"Could not request results from Google Speech Recognition service; {0}\".format(e))\n",
    "except:\n",
    "    pass;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4634cc96",
   "metadata": {},
   "source": [
    "# Converting Text to Speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "99179267",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install gTTS\n",
    "from gtts import gTTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "92681a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'Natural language processing (NLP) is an interdisciplinary subfield of linguistics, computer science, and artificial intelligence concerned with the interactions between computers and human language, in particular how to program computers to process and analyze large amounts of natural language data. The goal is a computer capable of \"understanding\" the contents of documents, including the contextual nuances of the language within them. The technology can then accurately extract information and insights contained in the documents as well as categorize and organize the documents themselves.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "9e50c1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#chooses the language, English('en')\n",
    "convert = gTTS(text=text, lang='en', slow=False)\n",
    "# Saving the converted audio in a mp3 file named\n",
    "convert.save(\"audio.mp3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "bea8a033",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'af': 'Afrikaans',\n",
       " 'ar': 'Arabic',\n",
       " 'bg': 'Bulgarian',\n",
       " 'bn': 'Bengali',\n",
       " 'bs': 'Bosnian',\n",
       " 'ca': 'Catalan',\n",
       " 'cs': 'Czech',\n",
       " 'da': 'Danish',\n",
       " 'de': 'German',\n",
       " 'el': 'Greek',\n",
       " 'en': 'English',\n",
       " 'es': 'Spanish',\n",
       " 'et': 'Estonian',\n",
       " 'fi': 'Finnish',\n",
       " 'fr': 'French',\n",
       " 'gu': 'Gujarati',\n",
       " 'hi': 'Hindi',\n",
       " 'hr': 'Croatian',\n",
       " 'hu': 'Hungarian',\n",
       " 'id': 'Indonesian',\n",
       " 'is': 'Icelandic',\n",
       " 'it': 'Italian',\n",
       " 'iw': 'Hebrew',\n",
       " 'ja': 'Japanese',\n",
       " 'jw': 'Javanese',\n",
       " 'km': 'Khmer',\n",
       " 'kn': 'Kannada',\n",
       " 'ko': 'Korean',\n",
       " 'la': 'Latin',\n",
       " 'lv': 'Latvian',\n",
       " 'ml': 'Malayalam',\n",
       " 'mr': 'Marathi',\n",
       " 'ms': 'Malay',\n",
       " 'my': 'Myanmar (Burmese)',\n",
       " 'ne': 'Nepali',\n",
       " 'nl': 'Dutch',\n",
       " 'no': 'Norwegian',\n",
       " 'pl': 'Polish',\n",
       " 'pt': 'Portuguese',\n",
       " 'ro': 'Romanian',\n",
       " 'ru': 'Russian',\n",
       " 'si': 'Sinhala',\n",
       " 'sk': 'Slovak',\n",
       " 'sq': 'Albanian',\n",
       " 'sr': 'Serbian',\n",
       " 'su': 'Sundanese',\n",
       " 'sv': 'Swedish',\n",
       " 'sw': 'Swahili',\n",
       " 'ta': 'Tamil',\n",
       " 'te': 'Telugu',\n",
       " 'th': 'Thai',\n",
       " 'tl': 'Filipino',\n",
       " 'tr': 'Turkish',\n",
       " 'uk': 'Ukrainian',\n",
       " 'ur': 'Urdu',\n",
       " 'vi': 'Vietnamese',\n",
       " 'zh-CN': 'Chinese (Simplified)',\n",
       " 'zh-TW': 'Chinese (Mandarin/Taiwan)',\n",
       " 'zh': 'Chinese (Mandarin)'}"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gtts\n",
    "gtts.lang.tts_langs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "20f61cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'In der Computerlinguistik (CL) oder linguistischen Datenverarbeitung (LDV) wird untersucht, wie natürliche Sprache in Form von Text- oder Sprachdaten mit Hilfe des Computers algorithmisch verarbeitet werden kann. Sie ist Schnittstelle zwischen Sprachwissenschaft und Informatik. In der englischsprachigen Literatur und Informatik ist neben dem Begriff natural language processing (NLP) auch computational linguistics (CL) gebräuchlich.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "5f6b40df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#chooses the language, English('en')\n",
    "convert = gTTS(text=text, lang='de' ,slow=False)\n",
    "# Saving the converted audio in a mp3 file named\n",
    "convert.save(\"audio.mp3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12311c97",
   "metadata": {},
   "source": [
    "# Translating Speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "542a84fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import goslate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "6f645c3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi world\n"
     ]
    }
   ],
   "source": [
    "text = \"Bonjour le monde\"\n",
    "gs = goslate.Goslate()\n",
    "translatedText = gs.translate(text,'en')\n",
    "print(translatedText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdfc501d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
